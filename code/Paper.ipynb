{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesizing Benchmarks for Predictive Modeling\n",
    "\n",
    "[Chris Cummins](http://chriscummins.cc/),\n",
    "[Pavlos Petoumenos](http://homepages.inf.ed.ac.uk/ppetoume/),\n",
    "[Zheng Wang](http://www.lancaster.ac.uk/staff/wangz3/),\n",
    "[Hugh Leather](http://homepages.inf.ed.ac.uk/hleather/).\n",
    "\n",
    "**Abstract**\n",
    "> Predictive modeling using machine learning is an effective method for building\n",
    "> compiler heuristics, but there is a shortage of benchmarks. Typical machine\n",
    "> learning experiments outside of the compilation field train over thousands or\n",
    "> millions of examples. In machine learning for compilers, however, there are\n",
    "> typically only a few dozen common benchmarks available. This limits the\n",
    "> quality of learned models, as they have very sparse training data for what are\n",
    "> often high-dimensional feature spaces. What is needed is a way to generate an\n",
    "> unbounded number of training programs that finely cover the feature space. At\n",
    "> the same time the generated programs must be similar to the types of programs\n",
    "> that human developers actually write, otherwise the learning will target the\n",
    "> wrong parts of the feature space.\n",
    ">\n",
    "> We mine open source repositories for program fragments and apply deep learning\n",
    "> techniques to automatically construct models for how humans write programs. We\n",
    "> then sample the models to generate an unbounded number of runnable training\n",
    "> programs, covering the feature space ever more finely. The quality of the\n",
    "> programs is such that even human developers struggle to distinguish our\n",
    "> generated programs from hand-written code.\n",
    ">\n",
    "> We use our generator for OpenCL programs, CLgen, to automatically synthesize\n",
    "> thousands of programs and show that learning over these improves the\n",
    "> performance of a state of the art predictive model by $1.27\\times$. In\n",
    "> addition, the fine covering of the feature space automatically exposes\n",
    "> weaknesses in the feature design which are invisible with the sparse training\n",
    "> examples from existing benchmark suites. Correcting these weaknesses further\n",
    "> increases performance by $4.30\\times$.\n",
    "\n",
    "**Keywords**  Synthetic program generation, OpenCL, Benchmarking, Deep Learning, GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a live research artifact. See [here](https://github.com/ChrisCummins/paper-synthesizing-benchmarks) for instructions to install this artifact on your own hardware. Please note that performance numbers obtained on hardware different to what we used may differ from those in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preamble\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%run lib/preamble.py\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"<style>\n",
    "/* hide index column in Pandas Data Frames */\n",
    "table.dataframe thead th:first-child { display: none; }\n",
    "table.dataframe tbody th { display: none; }\n",
    "\n",
    "/* tweak text size */\n",
    "div.text_cell_render { font-size: 1.3em; line-height: 1.4em; }\n",
    "</style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Predictive modeling is a well researched method for building optimization\n",
    "heuristics that often exceed human experts and reduces development time. A set\n",
    "of training programs are identified which are expected to be representative of\n",
    "the application domain. The programs are compiled and executed with different\n",
    "parameter values for the target heuristic, to determine which are the best\n",
    "values for each training program. Each program is also summarized by a vector of\n",
    "features which describe the information that is expected to be important in\n",
    "predicting the best heuristic parameter values. These training examples of\n",
    "program features and desired heuristic values are used to create a machine\n",
    "learning model which, when given the features from a new, unseen program, can\n",
    "predict good heuristic values for it.\n",
    "\n",
    "It is common for the feature vectors to contain dozens of elements. This means\n",
    "that a large volume of training data is needed to have an adequate sampling over\n",
    "the feature space. Without it, the machine learned models can only capture the\n",
    "coarse characteristics of the heuristic, and new programs which do not lie near\n",
    "to training points may be wrongly predicted. The accuracy of the machine learned\n",
    "heuristic is thus limited by the sparsity of the training points.\n",
    "\n",
    "There have been efforts to solve this problem using templates. The essence of\n",
    "the approach is to construct a probabilistic grammar with embedded semantic\n",
    "actions that defines a language of possible programs. New programs may be\n",
    "created by sampling the grammar and, through setting probabilities on the\n",
    "grammar productions, the sampling is biased towards producing programs from one\n",
    "part of the space or another. This technique is potentially completely general,\n",
    "since a grammar can theoretically be constructed to match any desired program\n",
    "domain. However, despite being theoretically possible, it is not easy to\n",
    "construct grammars which are both suitably general and also produce programs\n",
    "that are in any way similar to human written programs. It has been shown to be\n",
    "successful over a highly restricted space of stencil benchmarks with little\n",
    "control flow or program variability. But, it is not clear how much effort it\n",
    "will take, or even if it is possible for human experts to define grammars\n",
    "capable of producing human like programs in more complex domains.\n",
    "\n",
    "By contrast, our approach does not require an expert to define what human\n",
    "programs look like. Instead, we automatically infer the structure and likelihood\n",
    "of programs over a huge corpus of open source projects. From this corpus, we\n",
    "learn a probability distribution over sets of characters seen in human written\n",
    "code. Later, we sample from this distribution to generate new random programs\n",
    "which, because the distribution models human written code, are indistinguishable\n",
    "from human code. We can then populate our training data with an unbounded number\n",
    "of human like programs, covering the space with any desired granularity, far\n",
    "more finely than either existing benchmark suites, or even the corpus of open\n",
    "source projects. Our approach is enabled by two recent developments:\n",
    "\n",
    "The first is the breakthrough effectiveness of deep learning for modeling\n",
    "complex structure in natural languages. As we show, deep learning is capable not\n",
    "just of learning the macro syntactical and semantic structure of programs, but\n",
    "also the nuances of how humans typically write code. It is truly remarkable when\n",
    "one considers that it is given no prior knowledge of the syntax or semantics of\n",
    "the language.\n",
    "\n",
    "The second is the increasing popularity of public and open platforms for hosting\n",
    "software projects and source code. This popularity furnishes us with the\n",
    "thousands of programming examples that are necessary to feed into the deep\n",
    "learning. These open source examples are not, sadly, as useful for directly\n",
    "learning the compiler heuristics since they are not presented in a uniform,\n",
    "runnable manner, nor do they typically have extractable test data. Preparing\n",
    "each of the thousands of open source projects to be directly applicable for\n",
    "learning compiler heuristics would be an insurmountable task. In addition to our\n",
    "program generator, CLgen, we also provide an accompanying host driver which\n",
    "generates datasets for, then executes and profiles synthesized programs.\n",
    "\n",
    "We make the following contributions:\n",
    "\n",
    "-   We are the first to apply deep learning over source codes to\n",
    "    synthesize compilable, executable benchmarks.\n",
    "\n",
    "-   A novel tool CLgen for general-purpose benchmark synthesis using\n",
    "    deep learning. CLgen automatically generates thousands of human like\n",
    "    programs for use in predictive modeling.\n",
    "\n",
    "-   We use CLgen to automatically improve the performance of a state of\n",
    "    the art predictive model by 1.27x, and expose limitations in\n",
    "    the feature design of the model which, after correcting, further\n",
    "    increases performance by 2.66x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Motivation\n",
    "\n",
    "In this section we make the argument for synthetic benchmarks. We identified\n",
    "frequently used benchmark suites in a survey of research papers in the field\n",
    "of GPGPU performance tuning from top tier conferences between 2013–2016: CGO,\n",
    "HiPC, PACT, and PPoPP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/lit-review.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "benchmark_counts = [x[1] for x in data.groupby(\"Paper\").size().iteritems()]\n",
    "suites = pd.DataFrame(\n",
    "    [(x[0], x[1] / labmath.mean(benchmark_counts))\n",
    "     for x in data.groupby([\"Benchmark Suite\"]).size().iteritems()],\n",
    "    columns=[\"Benchmark Suite\", \"#. benchmarks per paper\"])\n",
    "suites.sort_values(\"#. benchmarks per paper\", inplace=True, ascending=False)\n",
    "\n",
    "print(\"Average number of benchmarks per paper:\", round(labmath.mean(benchmark_counts), 2))\n",
    "print(\"Median number of benchmarks per paper: \", round(labmath.median(benchmark_counts), 2))\n",
    "print()\n",
    "suites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2:**  The average number of benchmarks used in GPGPU research papers, organized by origin. In this work we use the seven most popular benchmark suites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=\"Benchmark Suite\", y=\"#. benchmarks per paper\", data=suites)\n",
    "# threshold for benchmark suites used in paper:\n",
    "plt.axhline(y=.5, color=\"k\", lw=1)\n",
    "plt.setp(ax.get_xticklabels(), rotation=90)\n",
    "plt.xlabel(\"\"); plt.ylabel(\"#. benchmarks used\")\n",
    "viz.finalise(figsize=(4, 2.25), tight=True)\n",
    "complete(msg=\"Replicated analysis of lit. review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected the 7 most frequently used benchmark suites (used in 92% of\n",
    "results), and evaluated the performance of the state of the art *Grewe et al.*\n",
    "predictive model across each. Benchmark data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amd_benchmarks = cgo13.LabelledData.from_csv('../data/amd-benchmarks.csv', group_by=\"suite\")\n",
    "amd_benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nvidia_benchmarks = cgo13.LabelledData.from_csv('../data/amd-benchmarks.csv', group_by=\"suite\")\n",
    "nvidia_benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Grewe et al.* model predicts whether running a given OpenCL kernel on the GPU gives better\n",
    "performance than on the CPU. The performance of a model trained on one benchmark\n",
    "suite and used to predict the optimal mapping for another suites is generally\n",
    "very poor.\n",
    "\n",
    "**Table 1:** Performance relative to the optimal of the *Grewe et al.* predictive model across different benchmark suites on an AMD GPU. The columns show the suite used for training; the rows show the suite used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = amd_benchmarks\n",
    "metrics = cgo13.classification(data, group_by=\"suite\")\n",
    "results = np.zeros((len(metrics), len(metrics[0])))\n",
    "\n",
    "for j in range(len(metrics)):\n",
    "    for i in range(len(metrics[j])):\n",
    "        results[j, i] = metrics[j][i].oracle * 100  # % optimal\n",
    "results = results.T\n",
    "\n",
    "groups = sorted(set(data[\"Group\"]))\n",
    "\n",
    "ax = sns.heatmap(results, annot=True, fmt=\".0f\", linewidths=.5,\n",
    "                 xticklabels=shortlabels(groups),\n",
    "                 yticklabels=shortlabels(groups), cbar=False,\n",
    "                 cmap=\"RdBu\", center=50, vmin=0, vmax=100)\n",
    "for t in ax.texts:\n",
    "    t.set_text(t.get_text() + \"%\")\n",
    "plt.xlabel(\"Benchmark suite used for training\")\n",
    "plt.ylabel(\"Benchmark suite used for testing\")\n",
    "viz.finalise(figsize=(6, 6), tight=True)\n",
    "complete(msg=\"Replicated cross-validation of predictive model across benchmark suites\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark suite which provides the best results, NVIDIA SDK,\n",
    "achieves on average only 49% of the optimal performance. The worst case is when\n",
    "training with Parboil to predict the optimal mappings for Polybench, where the\n",
    "model achieves only 11.5% of the optimal performance. It is clear that\n",
    "heuristics learned on one benchmark suite fail to generalize across other\n",
    "suites. This problem is caused both by the limited number of benchmarks contained in\n",
    "each suite, and the distribution of benchmarks within the feature space.\n",
    "\n",
    "**Figure 3a**: A two dimensional projection of the Grewe\n",
    "et al. predictive model over Parboil benchmarks on\n",
    "an NVIDIA GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = cgo13.model()\n",
    "parboil = nvidia_benchmarks[nvidia_benchmarks[\"benchmark\"].str.contains(r\"^parboil-\")]\n",
    "\n",
    "B_out = cgo13.xval_benchmarks(clf, parboil)\n",
    "num_correct = sum(B_out[\"p_correct\"])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(cgo13.raw_features(parboil))\n",
    "\n",
    "X = pca.transform(cgo13.raw_features(parboil))\n",
    "\n",
    "ax = plot_pca(X, B_out)\n",
    "\n",
    "xlim = ax.get_xlim()  # we'll use these for the next plot\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "viz.finalise(figsize=(2.5, 2.5), tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, there is a dense cluster of neighboring benchmarks, a smaller cluster\n",
    "of three benchmarks, and two outliers. The lack of neighboring observations\n",
    "means that the model is unable to learn a good heuristic for the two outliers,\n",
    "which leads to them being incorrectly optimized. Now we train the predictive\n",
    "model with additional benchmarks which are neighboring in the feature space.\n",
    "\n",
    "**Figure 3b:** Two outliers in (a) are incorrectly predicted due to the lack of nearby observations. The additional neighboring observations in (b) corrects this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = clone(clf)  # reset model\n",
    "\n",
    "additional_benchmarks = [\n",
    "    \"shoc-1.1.5-FFT-ifft1D_512\",\n",
    "    \"shoc-1.1.5-S3D-ratt6_kernel\",\n",
    "    \"shoc-1.1.5-S3D-ratx2_kernel\",\n",
    "    \"shoc-1.1.5-S3D-ratt9_kernel\",\n",
    "]\n",
    "\n",
    "additional = nvidia_benchmarks[nvidia_benchmarks[\"benchmark\"].str.contains(\n",
    "    \"|\".join(additional_benchmarks))]\n",
    "parboil_and_additional = nvidia_benchmarks[nvidia_benchmarks[\"benchmark\"].str.contains(\n",
    "    r\"^{}|{}\".format(\"parboil\", \"|\".join(additional_benchmarks)))]\n",
    "\n",
    "B_out = cgo13.xval_benchmarks(clf, parboil_and_additional, prefix=\"parboil\")\n",
    "\n",
    "num_correct_with_additional = sum(B_out[\"p_correct\"])\n",
    "\n",
    "X = pca.transform(cgo13.raw_features(parboil))\n",
    "\n",
    "ax = plot_pca(X, B_out, additional, pca)\n",
    "\n",
    "ax.set_xlim(xlim)  # use same axis as before\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "viz.finalise(figsize=(2.5, 2.5), tight=True)\n",
    "complete(num_correct_with_additional > num_correct,\n",
    "         \"Neighboring observations improves classification accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The addition of these observations (and the information they provide about that\n",
    "part of the feature space) causes the two outliers to be correctly optimized. We\n",
    "found such outliers in all of the benchmark suites.\n",
    "\n",
    "These results highlight the significant affect that the number and distribution\n",
    "of training programs has on the quality of predictive models. Without good\n",
    "coverage of the feature space, any machine learning methodology is unlikely to\n",
    "produce high quality heuristics, suitable for general use on arbitrary real\n",
    "applications. Our novel approach, described in the next section, solves this\n",
    "problem by generating an unbounded number of programs to cover the feature space\n",
    "at any desired granularity.\n",
    "\n",
    "# 3. Overview of Our Approach\n",
    "\n",
    "In this paper we present CLgen, a tool for synthesizing OpenCL benchmarks, and\n",
    "an accompanying host driver for executing synthetic benchmarks for gathering\n",
    "performance data for predictive modeling. While we demonstrate our approach\n",
    "using OpenCL, it is language agnostic. Our tool CLgen learns the semantics and\n",
    "structure from over a million lines of hand-written code from GitHub, and\n",
    "synthesizes programs through a process of iterative model sampling. We then use\n",
    "a host driver to execute the synthesized programs to gather performance data for\n",
    "use in predictive modeling. Our approach extends the state of the art by\n",
    "providing a general-purpose solution for benchmark synthesis, leading to better\n",
    "and more accurate predictive models.\n",
    "\n",
    "In the course of evaluating our technique against prior work we discovered that\n",
    "it is also useful for evaluating the quality of features. Since we are able to\n",
    "cover the space so much more finely than the prior work, which only used\n",
    "standard benchmark suites, we are able to find multiple programs with identical\n",
    "feature values but different best heuristic values. This indicates that the\n",
    "features are not sufficiently discriminative and should be extended with more\n",
    "information to allow those programs to be separated. We go on to show that doing\n",
    "this significantly increases the performance of the learned heuristics. We\n",
    "expect that our technique will be valuable for feature designers.\n",
    "\n",
    "# 4. CLgen: Benchmark Synthesis\n",
    "\n",
    "CLgen is an undirected, general-purpose program synthesizer for OpenCL. It\n",
    "adopts and augments recent advanced techniques from deep learning to learn over\n",
    "massive codebases. In contrast to existing grammar and template based\n",
    "approaches, CLgen is entirely probabilistic. It *learns* to program using neural\n",
    "networks which model the semantics and usage of a huge corpus of code fragments\n",
    "in the target programming language. This section describes the assembly of an\n",
    "OpenCL language corpus, the application of deep learning over this corpus, and\n",
    "the process of synthesizing programs.\n",
    "\n",
    "## 4.1. An OpenCL Language Corpus\n",
    "\n",
    "Deep learning requires large datasets. For the purpose of modeling a programming\n",
    "language, this means assembling a very large collection of real, hand-written\n",
    "source codes. We assembled OpenCL codes by mining public repositories on the\n",
    "popular code hosting site GitHub.\n",
    "\n",
    "This is itself a challenging task since OpenCL is an embedded language, meaning\n",
    "device code is often difficult to untangle and GitHub does not presently\n",
    "recognize it as a searchable programming language. We developed a search engine\n",
    "for the GitHub API which attempts to identify and download standalone OpenCL\n",
    "files through a process of file scraping and recursive header inlining.\n",
    "\n",
    "The dataset we mined (from June 2016):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import clgen.dbutil\n",
    "\n",
    "data = \"../data/github.db\"\n",
    "print(\"# content files:\", clgen.dbutil.num_rows_in(data, \"ContentFiles\"))\n",
    "print(\"line count:     \", clgen.dbutil.lc(data, \"ContentFiles\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly selected file *(replay cell to select a new file)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = clgen.dbutil.connect(\"../data/github.db\").cursor()\n",
    "db.execute(\"SELECT Contents FROM ContentFiles ORDER BY RANDOM() LIMIT 1\")\n",
    "print(db.fetchone()[0])\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prune the raw dataset extracted from GitHub using a custom toolchain we\n",
    "developed for rejection filtering and code rewriting, built on LLVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To replicate the mining of GitHub for OpenCL files:\n",
    "#\n",
    "# Generate an API token by following the instructions here:\n",
    "#   https://help.github.com/articles/creating-an-access-token-for-command-line-use/>\n",
    "# \n",
    "# From the command line, run:\n",
    "#\n",
    "#   $ export GITHUB_USERNAME=[your-username]\n",
    "#   $ export GITHUB_PW=[your-password]\n",
    "#   $ export GITHUB_TOKEN=[your-api-token]\n",
    "#   $ clgen-create-db --github github.db\n",
    "#   $ clgen-fetch-github github.db\n",
    "#\n",
    "# This process issues thousands of GitHub API requests per minute over the course of several hours.\n",
    "# Please exercise restrained in minimizing your use of this program -- we don't want to upset\n",
    "# the nice folks at GH :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rejection Filter\n",
    "\n",
    "The rejection filter accepts as input a content file and returns whether or not\n",
    "it contains compilable, executable OpenCL code. To do this we attempt to compile\n",
    "the input to NVIDIA PTX bytecode and perform static analysis to ensure a minimum\n",
    "static instruction count of three. We discard any inputs which do not compile or\n",
    "contain fewer than three instructions.\n",
    "\n",
    "Bytecode features of an example program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc = clgen.preprocess.compile_cl_bytecode(\"\"\"\n",
    "#define DTYPE float\n",
    "#define ALPHA(a) 3.5f * a\n",
    "inline DTYPE ax(DTYPE x) { return ALPHA(x); }\n",
    "\n",
    "__kernel void saxpy( /* SAXPY kernel */\n",
    "    __global DTYPE *input1,\n",
    "    __global DTYPE *input2,\n",
    "    const int nelem)\n",
    "{\n",
    "    unsigned int idx = get_global_id(0);\n",
    "    // = ax + y\n",
    "    if (idx < nelem) {\n",
    "        input2[idx] += ax(input1[idx]); }}\n",
    "\"\"\")\n",
    "        \n",
    "# extract features\n",
    "features = clgen.preprocess.bytecode_features(bc)\n",
    "\n",
    "print(clgen.format_json(features))\n",
    "complete(\"instructions_of_all_types\" in features, \"Bytecode feature extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During initial development it became apparent that isolating the OpenCL device\n",
    "code leads to a higher-than-expected discard rate (that is, seemingly valid\n",
    "OpenCL files being rejected). Through analyzing 148k lines of compilation\n",
    "errors, we discovered a large number of failures caused by undeclared\n",
    "identifiers — a result of isolating device code — 50% of undeclared identifier\n",
    "errors in the GitHub dataset were caused by only 60 unique identifiers. To\n",
    "address this, we developed a *shim header* which contains inferred values for\n",
    "common type definitions (e.g. `FLOAT_T`), and common constants (e.g. `WGSIZE`).\n",
    "\n",
    "**Listing 1:** The shim header file, providing inferred type aliases and constants for OpenCL on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import clgen.native\n",
    "print(readfile(clgen.native.SHIMFILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# without shim:\n",
    "try:\n",
    "    clgen.preprocess.preprocess(\"\"\"\n",
    "        __kernel void foo(__global float* a) {\n",
    "            a[get_global_id(0)] = ZERO;\n",
    "        }\n",
    "    \"\"\", use_shim=False)\n",
    "    complete(False, \"Code fails to compile without shim header\")\n",
    "except clgen.preprocess.BadCodeException:\n",
    "    complete(True, \"Code fails to compile without shim header\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "code = clgen.preprocess.preprocess(\"\"\"\n",
    "    __kernel void foo(__global float* a) {\n",
    "        a[get_global_id(0)] = ZERO;\n",
    "    }\n",
    "\"\"\", use_shim=True)\n",
    "\n",
    "print(code)\n",
    "complete(msg=\"Code compiles with shim header\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Injecting the shim decreases the discard rate from 40% to 32%, responsible for\n",
    "an additional 88k lines of code in the final language corpus. The resulting\n",
    "dataset is 2.0 million lines of compilable OpenCL source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linecount_of_good_files(data):\n",
    "    db = clgen.dbutil.connect(data).cursor()\n",
    "    db.execute(\"\"\"SELECT LC(ContentFiles.Contents) FROM ContentFiles\n",
    "               LEFT JOIN PreprocessedFiles ON ContentFiles.id=PreprocessedFiles.id\n",
    "               WHERE PreprocessedFiles.status=0\"\"\")\n",
    "    return db.fetchone()[0]\n",
    "\n",
    "data = \"../data/github.db\"\n",
    "\n",
    "# preprocess dataset\n",
    "# note results are cached. empty cache by uncommenting the next line (will take >30min):\n",
    "# clgen.dbutil.remove_preprocessed(data)\n",
    "clgen.preprocess.preprocess_db(data)\n",
    "\n",
    "num_good_files = clgen.dbutil.num_rows_in(data, \"PreprocessedFiles\", condition=\"WHERE status=0\")\n",
    "num_bad_files = clgen.dbutil.num_rows_in(data, \"PreprocessedFiles\", condition=\"WHERE status!=0\")\n",
    "ratio_bad = num_bad_files / (num_good_files + num_bad_files)\n",
    "\n",
    "print(\"# files in final dataset:\", num_good_files)\n",
    "print(\"# rejected files:        \", num_bad_files, \"({:.0f}%)\".format(ratio_bad * 100))\n",
    "print(\"line count of good files:\", linecount_of_good_files(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Rewriting\n",
    "\n",
    "Programming languages have few of the issues of semantic interpretation present\n",
    "in natural language, though there remains many sources of variance at the\n",
    "syntactic level. For example, the presence and content of comments in code, and\n",
    "the choice of identifying names given to variables. We consider these\n",
    "ambiguities to be *non-functional variance*, and developed a tool to normalize\n",
    "code of these variances so as to make the code more amenable to machine\n",
    "learning. This is a three step process:\n",
    "\n",
    "1.  The source is pre-processed to remove macros, conditional compilation, and\n",
    "    source comments.\n",
    "\n",
    "2.  Identifiers are rewritten to have a short but unique name based on their\n",
    "    order of appearance, using the sequential series $\\{a, b, c,\n",
    "    \\ldots, aa, ab, ac, \\ldots\\}$\n",
    "    for variables and $\\{A, B, C,\n",
    "    \\ldots, AA, AB, AC, \\ldots\\}$\n",
    "    for functions. This process isolates the syntactic structure of the code,\n",
    "    and unlike prior work, our rewrite method preserves program behavior.\n",
    "    Language built-ins (e.g. `get_global_id`, `asin`) are not rewritten.\n",
    "\n",
    "3.  A variant of the Google C++ code style is enforced to ensure consistent use\n",
    "    of braces, parentheses, and white space.\n",
    "    \n",
    "**Figure 5:**  The code rewriting process, which transforms code to make it more amenable to language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print(clgen.preprocess.preprocess(\"\"\"\n",
    "#define DTYPE float\n",
    "#define ALPHA(a) 3.5f * a\n",
    "inline DTYPE ax(DTYPE x) { return ALPHA(x); }\n",
    "\n",
    "__kernel void saxpy( /* SAXPY kernel */\n",
    "    __global DTYPE *input1,\n",
    "    __global DTYPE *input2,\n",
    "    const int nelem)\n",
    "{\n",
    "    unsigned int idx = get_global_id(0);\n",
    "    // = ax + y\n",
    "    if (idx < nelem) {\n",
    "        input2[idx] += ax(input1[idx]); }}\n",
    "\"\"\"))\n",
    "    complete(True, \"Preprocessing OpenCL code for machine learning\")\n",
    "except Exception as e:\n",
    "    complete(False, \"Preprocessing OpenCL code for machine learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly selected preprocessed file from dataset *(replay cell to select a new file)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = clgen.dbutil.connect(\"../data/github.db\").cursor()\n",
    "db.execute(\"SELECT Contents FROM PreprocessedFiles WHERE status=0 ORDER BY RANDOM() LIMIT 1\")\n",
    "print(db.fetchone()[0])\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A side effect of this process is a reduction in code size, largely due to the\n",
    "removal of comments and excess white space. The final language corpus contains\n",
    "1.3 million lines of transformed OpenCL, consisting of 9487 kernel functions.\n",
    "Identifier rewriting reduces the bag-of-words vocabulary size by 84%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import clgen.train\n",
    "\n",
    "db = clgen.dbutil.connect(\"../data/github.db\")\n",
    "clgen.train.create_corpus(db, \"../data/corpus.cl\", gh=True)\n",
    "\n",
    "lc, wc, cc = line_word_char_count(\"../data/corpus.cl\")\n",
    "print(\"  #. lines:\", lc)\n",
    "print(\"  #. words:\", wc)\n",
    "print(\"  #. chars:\", cc)\n",
    "complete(msg=\"Analysis of GitHub dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Learning OpenCL\n",
    "\n",
    "Generating valid, executable program code is an ambitious and\n",
    "challenging goal for machine learning. We employ state of the art deep\n",
    "language modeling techniques to achieve this task.\n",
    "\n",
    "We use the Long Short-Term Memory (LSTM) architecture of Recurrent Neural\n",
    "Network to learn a character-level language model over the corpus of OpenCL\n",
    "compute kernels. The LSTM network architecture comprises recurrent layers of\n",
    "*memory cells*, each consisting of an input, output, and forget gate, and an\n",
    "output layer providing normalized probability values from a 1-of-K coded\n",
    "vocabulary.\n",
    "\n",
    "We use a 3-layer LSTM network with 2048 nodes per layer. We train this\n",
    "17-million parameter model using *Stochastic Gradient Descent* for 50 epochs,\n",
    "using an initial learning rate of 0.002, decaying by a factor of one half every\n",
    "5 epochs. Training took three weeks on a single machine using an NVIDIA GTX\n",
    "Titan, with a final model size of 648MB. Training the network is a one-off cost,\n",
    "and can be parallelized across devices. The trained network can be deployed to\n",
    "lower-compute machines for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import clgen.model\n",
    "\n",
    "print(\"Training a tiny network on a tiny dataset:\")\n",
    "tiny_model = clgen.model.from_json({\n",
    "        \"corpus\": {\n",
    "            \"path\": \"../data/tiny-corpus\"\n",
    "        },\n",
    "        \"train_opts\": {\n",
    "            \"model_type\": \"lstm\",\n",
    "            \"rnn_size\": 128,\n",
    "            \"num_layers\": 2,\n",
    "            \"max_epochs\": 10\n",
    "        }\n",
    "    })\n",
    "tiny_model.cache.empty()\n",
    "print(tiny_model.corpus)\n",
    "tiny_model.train()\n",
    "tiny_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The model used in the paper (pre-trained):\")\n",
    "model = clgen.model.from_tar(\"../data/clgen-github-model-2016-nov-2048x3.tar.bz2\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Synthesizing OpenCL\n",
    "\n",
    "We synthesize OpenCL compute kernels by iteratively sampling the learned\n",
    "language model. We implemented two modes for model sampling: the first involves\n",
    "providing an *argument specification*, stating the data types and modifiers of\n",
    "all kernel arguments. When an argument specification is provided, the model\n",
    "synthesizes kernels matching this signature. In the second sampling mode this\n",
    "argument specification is omitted, allowing the model to synthesize compute\n",
    "kernels of arbitrary signatures, dictated by the distribution of argument types\n",
    "within the language corpus.\n",
    "\n",
    "In either mode we generate a *seed* text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(clgen.sampler.serialize_argspec([\n",
    "    '__global float*',\n",
    "    '__global float*',\n",
    "    '__global float*',\n",
    "    'const int'\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(clgen.sampler.serialize_argspec([\n",
    "    '__global int*',\n",
    "    'const int'\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(clgen.sampler.serialize_argspec([\n",
    "    '__global int*',\n",
    "    'const int'\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then sample the model, character by character, until the end of the compute kernel is reached, or until a\n",
    "predetermined maximum number of characters is reached.\n",
    "\n",
    "Live demos (re-run to re-sample):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import clgen.sampler\n",
    "\n",
    "argspec = ['__global float*', '__global float*', '__global float*', 'const int']\n",
    "\n",
    "sampler = clgen.sampler.from_json({\n",
    "        \"kernels\": { \n",
    "            \"args\": argspec,\n",
    "            \"max_length\": 300,\n",
    "        },\n",
    "        \"sampler\": {\n",
    "            \"batch_size\": 1,\n",
    "            \"max_batches\": 1\n",
    "        }\n",
    "    })\n",
    "\n",
    "print(\"Sample from the tiny model we just trained:\")\n",
    "sampler.sample(tiny_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Sample from the model used in the paper:\")\n",
    "sampler.sample(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rejection filter then either accepts or rejects the sample as a candidate synthetic benchmark\n",
    "\n",
    "Live demo which generates a single statically-verified OpenCL kernel using the pre-trained network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Generate a statically checked kernel using the pre-trained network:\")\n",
    "sampler = clgen.sampler.from_json({\n",
    "        \"kernels\": { \n",
    "            \"args\": argspec,\n",
    "            \"max_length\": 500,\n",
    "        },\n",
    "        \"sampler\": {\n",
    "            \"batch_size\": 5,\n",
    "            \"max_kernels\": 1\n",
    "        }\n",
    "    })\n",
    "\n",
    "sampler.cache(model).empty()\n",
    "sampler.sample(model)\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Benchmark Execution\n",
    "\n",
    "We developed a host driver to gather performance data from synthesized CLgen\n",
    "code. The driver accepts as input an OpenCL kernel, generates *payloads* of\n",
    "user-configurable sizes, and executes the kernel using the generated payloads,\n",
    "providing dynamic checking of kernel behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import clgen.cldrive, clgen.preprocess\n",
    "\n",
    "src = clgen.preprocess.preprocess(\"\"\"\n",
    "__kernel void benchmark(__global float* a, __global float* b, const float c, const int n) {\n",
    "   const int i = get_global_id(0);\n",
    "   if (i < n)\n",
    "     a[i] += c * b[i];\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "if has_opencl():\n",
    "    ctx, queue = clgen.cldrive.init_opencl()\n",
    "    driver = clgen.cldrive.KernelDriver(ctx, src)\n",
    "    print(driver)\n",
    "else:\n",
    "    print(\"no OpenCL support, skipped\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Generating Payloads\n",
    "\n",
    "A *payload* encapsulates all of the arguments of an OpenCL compute kernel. After\n",
    "parsing the input kernel to derive argument types, a rule-based approach is used\n",
    "to generate synthetic payloads. For a given global size $S_g$: host buffers of\n",
    "$S_g$ elements are allocated and populated with random values for global pointer\n",
    "arguments, device-only buffers of $S_g$ elements are allocated for local pointer\n",
    "arguments, integral arguments are given the value $S_g$, and all other scalar\n",
    "arguments are given random values. Host to device data transfers are enqueued\n",
    "for all non-write-only global buffers, and all non-read-only global buffers are\n",
    "transferred back to the host after kernel execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if has_opencl():\n",
    "    size = 4096\n",
    "    payload = clgen.cldrive.KernelPayload.create_random(driver, size)\n",
    "    print(payload)\n",
    "else:\n",
    "    print(\"no OpenCL support, skipped\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Dynamic Checker\n",
    "\n",
    "For the purpose of performance benchmarking we are not interested in the\n",
    "correctness of computed values, but we define a class of programs as performing\n",
    "*useful work* if they predictably compute some result. We devised a low-overhead\n",
    "runtime behavior check to validate that a synthesized program does useful work\n",
    "based on the outcome of four executions of a tested program:\n",
    "\n",
    "**Step 1:** Create 4 equal size payloads $A_{1in}$, $B_{1in}$, $A_{2in}$, $B_{2in}$,\n",
    "    subject to restrictions: $A_{1in}=A_{2in}$, $B_{1in}=B_{2in}$, $A_{1in} \\ne\n",
    "    B_{1in}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "if has_opencl():\n",
    "    # create payloads\n",
    "    A1in = clgen.cldrive.KernelPayload.create_sequential(driver, size)\n",
    "    A2in = deepcopy(A1in)\n",
    "\n",
    "    B1in = clgen.cldrive.KernelPayload.create_random(driver, size)\n",
    "    B2in = deepcopy(B1in)\n",
    "\n",
    "    # check contstraints\n",
    "    assert(A1in == A2in)\n",
    "    assert(B1in == B2in)\n",
    "    assert(A1in != B1in)\n",
    "    passed()\n",
    "else:\n",
    "    print(\"no OpenCL support, skipped\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Execute kernel $k$ 4 times: $k(A_{1in}) \\rightarrow A_{1out}$, $k(B_{1in})\n",
    "   \\rightarrow B_{1out}$, $k(A_{2in}) \\rightarrow A_{2out}$, $k(B_{2in})\n",
    "   \\rightarrow B_{2out}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if has_opencl():\n",
    "    k = partial(driver, queue)\n",
    "    A1out = k(A1in)\n",
    "    B1out = k(B1in)\n",
    "    A2out = k(A2in)\n",
    "    B2out = k(B2in)\n",
    "else:\n",
    "    print(\"no OpenCL support, skipped\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:**  Assert:\n",
    "\n",
    "* $A_{1out} \\ne A_{1in}$ and $B_{1out} \\ne B_{1in}$, else $k$ has no output (for these inputs).\n",
    "\n",
    "* $A_{1out} \\ne B_{1out}$ and $A_{2out} \\ne B_{2out}$, else $k$ is input insensitive (for these inputs).\n",
    "\n",
    "* $A_{1out}=A_{2out}$ and $B_{1out}=B_{2out}$, else $k$ is non-deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if has_opencl():\n",
    "    # outputs must be consistent across runs:\n",
    "    assert(A1out == A2out)\n",
    "    assert(B1out == B2out)\n",
    "\n",
    "    # outputs must depend on inputs:\n",
    "    if any(not x.is_const for x in driver.prototype.args):\n",
    "        assert(A1out != B1out)\n",
    "\n",
    "    # outputs must be different from inputs:\n",
    "    assert(A1in != A1out)\n",
    "    assert(B1in != B1out)\n",
    "else:\n",
    "    print(\"no OpenCL support, skipped\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equality checks for floating point values are performed with an appropriate\n",
    "epsilon to accommodate rounding errors, and a timeout threshold is also used to\n",
    "catch kernels which are non-terminating. Our method is based on random\n",
    "differential testing, though we emphasize that this is not a general purpose\n",
    "approach and is tailored specifically for our use case. For example, we\n",
    "anticipate a false positive rate for kernels with subtle sources of\n",
    "non-determinism which more thorough methods may expose, however we deemed such\n",
    "methods unnecessary for our purpose of performance modeling.\n",
    "\n",
    "Examples of programs which are our dynamic checker catches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driver = clgen.cldrive.KernelDriver(ctx, \"\"\"\n",
    "__kernel void A(__global int* a) {}\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    driver.validate(queue, 8192)\n",
    "except clgen.cldrive.E_NO_OUTPUTS:\n",
    "    print(\"caught kernel with no outputs\")\n",
    "except clgen.cldrive.OpenCLNotSupported:\n",
    "    print(\"no OpenCL support, skipped\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driver = clgen.cldrive.KernelDriver(ctx, \"\"\"\n",
    "__kernel void A(__global int* a) {\n",
    "  int b = get_global_id(0);\n",
    "  a[b] = 0;\n",
    "}\"\"\")\n",
    "\n",
    "try:\n",
    "    driver.validate(queue, 8192)\n",
    "except clgen.cldrive.E_INPUT_INSENSITIVE:\n",
    "    print(\"caught input-insensitive kernel\")\n",
    "except clgen.cldrive.OpenCLNotSupported:\n",
    "    print(\"no OpenCL support, skipped\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driver = clgen.cldrive.KernelDriver(ctx, \"\"\"\n",
    "__kernel void A(__global float* a) {\n",
    "  int b = get_global_id(0);\n",
    "\n",
    "  a[b] = a[b-1] + a[b+1]; /* race condition */\n",
    "}\"\"\")\n",
    "\n",
    "try:\n",
    "    driver.validate(queue, 8192)\n",
    "except clgen.cldrive.E_NONDETERMINISTIC:\n",
    "    print(\"caught non-deterministic kernel\")\n",
    "except clgen.cldrive.OpenCLNotSupported:\n",
    "    print(\"no OpenCL support, skipped\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluation of Synthetic Programs\n",
    "\n",
    "In this section we evaluate the quality of programs synthesized by CLgen\n",
    "by their likeness to hand-written code, and discuss limitations of the\n",
    "synthesis and execution pipeline.\n",
    "\n",
    "## 6.1. Likeness to hand-written code\n",
    "\n",
    "Judging whether a source code was written by a human is a challenging task for a\n",
    "machine, so we adopt a methodology from machine learning research based on the\n",
    "*Turing Test*. We reason that if the output of CLgen is human like code, then a\n",
    "human judge will be unable to distinguish it from hand-written code.\n",
    "\n",
    "We devised a double blind test in which 15 volunteer OpenCL developers from\n",
    "industry and academia were shown 10 OpenCL kernels each. Participants were\n",
    "tasked with judging whether, for each kernel, they believed it to have been\n",
    "written by hand or by machine. Kernels were randomly selected for each\n",
    "participant from two equal sized pools of synthetically generated and hand-\n",
    "written code from GitHub.\n",
    "\n",
    "The participants were divided into two groups, with 10 of them receiving code\n",
    "generated by CLgen, and 5 of them acting as a control group, receiving code\n",
    "generated by CLSmith, a program generator for differential testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/turing-test.csv\")\n",
    "data[\"Score (%)\"] = (data[\"True Positive\"] + data[\"True Negative\"]) * 10\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of participant’s answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clgen_data   = data[data[\"Synthetic Source\"].str.contains(\"CLgen\")]\n",
    "clsmith_data = data[data[\"Synthetic Source\"].str.contains(\"CLSmith\")]\n",
    "\n",
    "print(\"Mean CLgen score:  \", round(  clgen_data[\"Score (%)\"].mean(), 1), \"%, stdev:\",\n",
    "      round(  clgen_data[\"Score (%)\"].std(), 1), \"%\")\n",
    "print(\"Mean CLSmith score:\", round(clsmith_data[\"Score (%)\"].mean(), 1), \"%, stdev:\",\n",
    "      round(clsmith_data[\"Score (%)\"].std(), 1), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This an unsurprising outcome as generated programs for\n",
    "testing have multiple “tells”, for example, their only input is a single `ulong`\n",
    "pointer. This demonstrates that CLgen code is indistinguishable from hand-written\n",
    "programs, with human judges scoring no better than random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An online game of this test is available [here](http://humanorrobot.uk/game/?g=opencl&m=nitt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Limitations\n",
    "\n",
    "Our new approach enables the synthesis of more human-like programs than current\n",
    "state of the art program generators, and without the expert guidance required by\n",
    "template based generators, but it has limitations. Currently we only run single-\n",
    "kernel benchmarks, and our method of seeding the language models with the start\n",
    "of a function means that we cannot support user defined types, or calls to user-\n",
    "defined functions. The first limitation will be overcome by extending the host\n",
    "driver to explore multi-kernel schedules and interleaving of kernel executions.\n",
    "The second limitation can be addressed through recursive program synthesis,\n",
    "whereby a call to a user-defined function or type will trigger candidate\n",
    "functions and types to be synthesized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Experimental Methodology\n",
    "\n",
    "## 7.1. Experimental Setup \n",
    "\n",
    "#### Predictive Model\n",
    "\n",
    "We reproduce the predictive model from Grewe et al. (2013). The predictive model\n",
    "is used to determine the optimal mapping of a given OpenCL kernel to either a\n",
    "GPU or CPU. It uses supervised learning to construct a decision tree with a\n",
    "combination of static and dynamic kernel features extracted from source code and\n",
    "the OpenCL runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def features_to_dict(path):\n",
    "    import clgen.features\n",
    "    buf = StringIO()\n",
    "    clgen.features.feature_headers(file=buf)  # feature names\n",
    "    keys = buf.getvalue().rstrip().split(\",\")\n",
    "    buf = StringIO()\n",
    "    clgen.features.features(path, file=buf)  # extract features\n",
    "    values = buf.getvalue().rstrip().split(\",\")\n",
    "    return dict((k, v) for k, v in zip(keys, values))\n",
    "\n",
    "# kernel to get features from:\n",
    "kernel = \"\"\"\n",
    "__kernel void foobar(__global float *a, __global float *b, const int n) {\n",
    "    unsigned int i = get_global_id(0);\n",
    "    if (i < n)\n",
    "        b[i] += a[i] * 2; \n",
    "}\n",
    "\"\"\"\n",
    "    \n",
    "with open(\"../data/kernel.cl\", \"w\") as outfile:\n",
    "    print(kernel, file=outfile)\n",
    "\n",
    "DictTable(features_to_dict(\"../data/kernel.cl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Benchmarks\n",
    "\n",
    "As in Grewe et al., we test our model on the NAS Parallel Benchmarks (NPB). We\n",
    "use the hand-optimized OpenCL implementation of Seo (2011). In Grewe et al. the\n",
    "authors augment the training set of the predictive model with 47 additional\n",
    "kernels taken from 4 GPGPU benchmark suites. To more fully sample the program\n",
    "space, we use a much larger collection of 142 programs. These additional\n",
    "programs are taken from all 7 of the most frequently used benchmark suites\n",
    "identified in the motivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "benchmarks = sorted(set(amd_benchmarks[\"benchmark\"]))\n",
    "\n",
    "print(len(benchmarks), \"benchmark kernels:\")\n",
    "for benchmark in benchmarks:\n",
    "    print(\" \", benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We synthesized 1,000 kernels with CLgen to use as additional benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clgen_kernels = fs.ls(\"../data/clgen-1000/kernels\")\n",
    "\n",
    "print(len(clgen_kernels), \"CLgen kernels:\")\n",
    "for kernel in clgen_kernels:\n",
    "    print(\" \", kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Platforms\n",
    "\n",
    "We evaluate our approach on two 64-bit CPU-GPU systems. One system has an AMD\n",
    "GPU and uses OpenSUSE 12.3; the other is equipped with an NVIDIA GPU and uses\n",
    "Ubuntu 16.04. Both platforms were unloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import clgen.clutil\n",
    "clgen.clutil.platform_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets\n",
    "\n",
    "The NPB and Parboil benchmark suites are packaged with multiple datasets. We use\n",
    "all of the packaged datasets (5 per program in NPB, 1-4 per program in Parboil).\n",
    "For all other benchmarks, the default datasets are used. We configured the CLgen\n",
    "host driver to synthesize payloads between 128B-130MB, approximating that of the\n",
    "dataset sizes found in the benchmark programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Methodology\n",
    "\n",
    "We replicated the methodology of Grewe et al. Each experiment is repeated five\n",
    "times and the average execution time is recorded. The execution time includes\n",
    "both device compute time and the data transfer overheads.\n",
    "\n",
    "We use *leave-one-out cross-validatation* to evaluate predictive models. For\n",
    "each benchmark, a model is trained on data from all other benchmarks and used to\n",
    "predict the mapping for each kernel and dataset in the excluded program. We\n",
    "repeat this process with and without the addition of synthetic benchmarks in the\n",
    "training data. We do not test model prediction on synthetic benchmarks.\n",
    "\n",
    "Reproducing experiments on the AMD APP SDK benchmark suites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if can_reproduce_experiments():\n",
    "    !rm -fv ../data/benchmarks/*.csv\n",
    "    !cd benchmarks && ./mkdata\n",
    "else:\n",
    "    print(\"platform does not have the requirements to run this experiment\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = None\n",
    "if can_reproduce_experiments():\n",
    "    data = pd.read_csv(\"../data/benchmarks/training.csv\")\n",
    "    \n",
    "    ax = sns.barplot(x=\"benchmark\", y=\"speedup\", data=data)\n",
    "    plt.ylabel(\"Speedup best over worst mapping\")\n",
    "    plt.xlabel(\"AMD SDK Benchmark kernels\")\n",
    "    plt.axhline(y=1, color=\"k\", lw=1)  # speedup line\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90)  # rotate x ticks\n",
    "    ax.set_xticklabels([shortbenchmark(x.get_text()) for x in ax.get_xticklabels()])\n",
    "    viz.finalise(figsize=(9,4))\n",
    "else:\n",
    "    print(\"no new data, skipped\", file=sys.stderr)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproducing experiments on a 1% subset of CLgen kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if can_reproduce_experiments():\n",
    "    !rm -fv ../data/clgen-10/*.csv\n",
    "    !cd bin && ./mkdata\n",
    "else:\n",
    "    print(\"platform does not have the requirements to run this experiment\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = None\n",
    "if can_reproduce_experiments():\n",
    "    data = pd.read_csv(\"../data/clgen-10/training.csv\")\n",
    "    \n",
    "    ax = sns.barplot(x=\"benchmark\", y=\"speedup\", ci=95, data=data)\n",
    "    plt.ylabel(\"Speedup (95% CI across datasets)\")\n",
    "    plt.xlabel(\"CLgen kernels\")\n",
    "    plt.axhline(y=1, color=\"k\", lw=1)  # speedup line\n",
    "    ax.set_xticklabels(range(1, len(data) + 1))\n",
    "    viz.finalise(figsize=(9,4))\n",
    "else:\n",
    "    print(\"no new data, skipped\", file=sys.stderr)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Experimental Results\n",
    "\n",
    "We evaluate the effectiveness of our approach on two heterogeneous systems. We\n",
    "first compare the performance of a state of the art predictive model Grewe et\n",
    "al. with and without the addition of synthetic benchmarks, then show how the\n",
    "synthetic benchmarks expose weaknesses in the model’s design and how these can\n",
    "be addressed to develop a better model. Finally we compare the ability of CLgen\n",
    "to explore the program feature space against a state of the art program\n",
    "generator.\n",
    "\n",
    "## 8.1. Performance Evaluation\n",
    "\n",
    "Performance data from 1,000 CLgen kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amd_clgen = pd.read_csv(\"../data/amd-clgen.csv\")\n",
    "amd_clgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nvidia_clgen = pd.read_csv(\"../data/nvidia-clgen.csv\")\n",
    "nvidia_clgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 7:** Speedup of programs using *Grewe et al.* predictive model with and without synthetic benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def speedups_with_clgen(platform):\n",
    "    # datasets: B - benchmarks, S - synthetics, BS - benchmarks + synthetics:\n",
    "    B = pd.read_csv(\"../data/{platform}-benchmarks.csv\".format(platform=platform))\n",
    "    B[\"group\"] = [\"B\"] * len(B)\n",
    "\n",
    "    S = pd.read_csv(\"../data/{platform}-clgen.csv\".format(platform=platform))\n",
    "    S[\"group\"] = [\"S\"] * len(S)\n",
    "\n",
    "    BS = pd.concat((B, S))\n",
    "\n",
    "    # find the ZeroR. This is the device which is most frequently optimal\n",
    "    Bmask = B[B[\"benchmark\"].str.contains(\"npb-3.3-\")]\n",
    "    zeror = Counter(Bmask[\"oracle\"]).most_common(1)[0][0]\n",
    "    zeror_runtime = \"runtime_\" + zeror.lower()\n",
    "\n",
    "    # get the names of the benchmarks, in the form: $suite-$version-$benchmark\n",
    "    benchmark_names = sorted(set([\n",
    "        re.match(r\"^([^0-9]+-[0-9\\.]+-[^-]+)-\", b).group(1)\n",
    "        for b in B[\"benchmark\"] if b.startswith(\"npb-\")\n",
    "    ]))\n",
    "\n",
    "    B_out, BS_out = [], []\n",
    "    for benchmark in benchmark_names:\n",
    "        clf = cgo13.model()\n",
    "        features = get_cgo13_features\n",
    "        # cross validate on baseline\n",
    "        B_out += cgo13.leave_one_benchmark_out(clf, features, B, benchmark)\n",
    "        # reset model\n",
    "        clf = cgo13.model()\n",
    "        # repeate cross-validation with synthetic kernels\n",
    "        BS_out += cgo13.leave_one_benchmark_out(clf, features, BS, benchmark)\n",
    "\n",
    "    # create results frame\n",
    "    R_out = []\n",
    "    for b, bs in zip(B_out, BS_out):\n",
    "        # get runtimes of device using predicted device\n",
    "        b_p_runtime = b[\"runtime_\" + b[\"p\"].lower()]\n",
    "        bs_p_runtime = bs[\"runtime_\" + bs[\"p\"].lower()]\n",
    "\n",
    "        # speedup is the ratio of runtime using the predicted device\n",
    "        # over runtime using ZeroR device\n",
    "        b[\"p_speedup\"] = b_p_runtime / b[zeror_runtime]\n",
    "        bs[\"p_speedup\"] = bs_p_runtime / bs[zeror_runtime]\n",
    "\n",
    "        # get the group label, in the form $benchmark.$dataset\n",
    "        group = re.sub(r\"[^-]+-[0-9\\.]+-([^-]+)-.+\", r\"\\1\",\n",
    "                       b[\"benchmark\"]) + \".\" + b[\"dataset\"]\n",
    "        b[\"group\"] = group\n",
    "        bs[\"group\"] = group\n",
    "\n",
    "        # set the training data type\n",
    "        b[\"training\"] = \"Grewe et al.\"\n",
    "        bs[\"training\"] = \"w. CLgen\"\n",
    "\n",
    "        R_out.append(b)\n",
    "        R_out.append(bs)\n",
    "\n",
    "    R = pd.DataFrame(R_out)\n",
    "\n",
    "    b_mask = R[\"training\"] == \"Grewe et al.\"\n",
    "    bs_mask = R[\"training\"] == \"w. CLgen\"\n",
    "\n",
    "    B_speedup = labmath.mean(R[b_mask].groupby([\"group\"])[\"p_speedup\"].mean())\n",
    "    BS_speedup = labmath.mean(R[bs_mask].groupby([\"group\"])[\"p_speedup\"].mean())\n",
    "\n",
    "    print(\"Results on {}:\".format(platform.upper()))\n",
    "    print(\"  #. benchmarks:                  \",\n",
    "          len(set(B[\"benchmark\"])), \"kernels,\", len(B), \"observations\")\n",
    "    print(\"  #. synthetic:                   \",\n",
    "          len(set(S[\"benchmark\"])), \"kernels,\", len(S), \"observations\")\n",
    "    print()\n",
    "    print(\"  ZeroR device:                    {}\".format(zeror))\n",
    "    print()\n",
    "    print(\"  Speedup of Grewe et al.:         {:.2f} x\".format(B_speedup))\n",
    "    print(\"  Speedup w. CLgen:                {:.2f} x\".format(BS_speedup))\n",
    "\n",
    "    R = R.append({  # average bars\n",
    "        \"group\": \"Average\",\n",
    "        \"p_speedup\": B_speedup,\n",
    "        \"training\": \"Grewe et al.\"\n",
    "    }, ignore_index=True)\n",
    "    R = R.append({\n",
    "        \"group\": \"Average\",\n",
    "        \"p_speedup\": BS_speedup,\n",
    "        \"training\": \"w. CLgen\"\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    R[\"p_speedup\"] -= 1  # negative offset so that bars start at 1\n",
    "\n",
    "    # colors\n",
    "    palette = sns.cubehelix_palette(len(set(R[\"training\"])),\n",
    "                                    rot=-.4, light=.85, dark=.35)\n",
    "\n",
    "    ax = sns.barplot(\n",
    "        x=\"group\", y=\"p_speedup\", data=R, ci=None, hue=\"training\",\n",
    "        palette=palette)\n",
    "    plt.ylabel(\"Speedup\")\n",
    "    plt.xlabel(\"\")\n",
    "\n",
    "    plt.axhline(y=0, color=\"k\", lw=1)  # speedup line\n",
    "    plt.axvline(x=plt.xlim()[1] - 1, color=\"k\", lw=1, linestyle=\"--\")  # average line\n",
    "\n",
    "    ax.get_legend().set_title(\"\")  # no legend title\n",
    "    plt.legend(loc='upper right')\n",
    "    ax.get_legend().draw_frame(True)\n",
    "\n",
    "    # plot shape and size\n",
    "    figsize = (9, 2.2)\n",
    "    if platform == \"nvidia\":\n",
    "        typecast = int; plt.ylim(-1, 16)\n",
    "    else:\n",
    "        typecast = float\n",
    "\n",
    "    # counter negative offset:\n",
    "    ax.set_yticklabels([typecast(i) + 1 for i in ax.get_yticks()])\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "    viz.finalise(figsize=figsize, tight=True)\n",
    "\n",
    "speedups_with_clgen(\"amd\")\n",
    "speedups_with_clgen(\"nvidia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speedups of the *Grewe et al. *predictive model over the NAS Parallel Benchmark\n",
    "suite with and without the addition of synthesized benchmarks for training.\n",
    "Speedups are calculated relative to the best single-device mapping for each\n",
    "experimental platform, which is CPU-only for AMD and GPU-only for NVIDIA. The\n",
    "fine grained coverage of the feature space which synthetic benchmarks provide\n",
    "improves performance dramatically for the NAS benchmarks. Across both systems,\n",
    "we achieve an average speedup of 2.42x with the addition of synthetic\n",
    "benchmarks, with prediction improvements over the baseline for 62.5% of\n",
    "benchmarks on AMD and 53.1% on NVIDIA.\n",
    "\n",
    "The strongest performance improvements are on NVIDIA with the `FT` benchmark, a\n",
    "benchmark which suffers greatly under a single-device mapping. However, the\n",
    "performance on AMD for the same benchmark slightly degrades after adding the\n",
    "synthetic benchmarks, which we address in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this experiment on the small subset of data we produced earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if can_reproduce_experiments():\n",
    "    # datasets: B - benchmarks, S - synthetics, BS - benchmarks + synthetics:\n",
    "    B = pd.read_csv(\"../data/benchmarks/training.csv\")\n",
    "    B[\"group\"] = [\"B\"] * len(B)\n",
    "\n",
    "    S = pd.read_csv(\"../data/clgen-10/training.csv\")\n",
    "    S[\"group\"] = [\"S\"] * len(S)\n",
    "\n",
    "    BS = pd.concat((B, S))\n",
    "\n",
    "    # find the ZeroR. This is the device which is most frequently optimal\n",
    "    zeror = Counter(B[\"oracle\"]).most_common(1)[0][0]\n",
    "    zeror_runtime = \"runtime_\" + zeror.lower()\n",
    "\n",
    "    # get the names of the benchmarks, in the form: $suite-$version-$benchmark\n",
    "    benchmark_names = sorted(set([\n",
    "        re.match(r\"^([^0-9]+-[0-9\\.]+-[^-]+)-\", b).group(1)\n",
    "        for b in B[\"benchmark\"]\n",
    "    ]))\n",
    "\n",
    "    B_out, BS_out = [], []\n",
    "    for benchmark in benchmark_names:\n",
    "        clf = cgo13.model()\n",
    "        features = get_cgo13_features\n",
    "        # cross validate on baseline\n",
    "        B_out += cgo13.leave_one_benchmark_out(clf, features, B, benchmark)\n",
    "        # reset model\n",
    "        clf = cgo13.model()\n",
    "        # repeate cross-validation with synthetic kernels\n",
    "        BS_out += cgo13.leave_one_benchmark_out(clf, features, BS, benchmark)\n",
    "\n",
    "    # create results frame\n",
    "    R_out = []\n",
    "    for b, bs in zip(B_out, BS_out):\n",
    "        # get runtimes of device using predicted device\n",
    "        b_p_runtime = b[\"runtime_\" + b[\"p\"].lower()]\n",
    "        bs_p_runtime = bs[\"runtime_\" + bs[\"p\"].lower()]\n",
    "\n",
    "        # speedup is the ratio of runtime using the predicted device\n",
    "        # over runtime using ZeroR device\n",
    "        b[\"p_speedup\"] = b_p_runtime / b[zeror_runtime]\n",
    "        bs[\"p_speedup\"] = bs_p_runtime / bs[zeror_runtime]\n",
    "\n",
    "        # get the group label, in the form $benchmark.$dataset\n",
    "        group = escape_benchmark_name(b[\"benchmark\"])\n",
    "        b[\"group\"] = group\n",
    "        bs[\"group\"] = group\n",
    "\n",
    "        # set the training data type\n",
    "        b[\"training\"] = \"Grewe et al.\"\n",
    "        bs[\"training\"] = \"w. CLgen\"\n",
    "\n",
    "        R_out.append(b)\n",
    "        R_out.append(bs)\n",
    "\n",
    "    R = pd.DataFrame(R_out)\n",
    "    b_mask = R[\"training\"] == \"Grewe et al.\"\n",
    "    bs_mask = R[\"training\"] == \"w. CLgen\"\n",
    "\n",
    "    B_speedup = labmath.mean(R[b_mask].groupby([\"group\"])[\"p_speedup\"].mean())\n",
    "    BS_speedup = labmath.mean(R[bs_mask].groupby([\"group\"])[\"p_speedup\"].mean())\n",
    "\n",
    "    print(\"Results:\")\n",
    "    print(\"  #. benchmarks:                  \",\n",
    "          len(set(B[\"benchmark\"])), \"kernels,\", len(B), \"observations\")\n",
    "    print(\"  #. synthetic:                   \",\n",
    "          len(set(S[\"benchmark\"])), \"kernels,\", len(S), \"observations\")\n",
    "    print()\n",
    "    print(\"  ZeroR device:                    {}\".format(zeror))\n",
    "    print()\n",
    "    print(\"  Speedup of Grewe et al.:         {:.2f} x\".format(B_speedup))\n",
    "    print(\"  Speedup w. CLgen:                {:.2f} x\".format(BS_speedup))\n",
    "\n",
    "    R = R.append({  # average bars\n",
    "        \"group\": \"Average\",\n",
    "        \"p_speedup\": B_speedup,\n",
    "        \"training\": \"Grewe et al.\"\n",
    "    }, ignore_index=True)\n",
    "    R = R.append({\n",
    "        \"group\": \"Average\",\n",
    "        \"p_speedup\": BS_speedup,\n",
    "        \"training\": \"w. CLgen\"\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    R[\"p_speedup\"] -= 1  # negative offset so that bars start at 1\n",
    "\n",
    "    # colors\n",
    "    palette = sns.cubehelix_palette(len(set(R[\"training\"])),\n",
    "                                    rot=-.4, light=.85, dark=.35)\n",
    "\n",
    "    ax = sns.barplot(\n",
    "        x=\"group\", y=\"p_speedup\", data=R, ci=None, hue=\"training\",\n",
    "        palette=palette)\n",
    "    plt.ylabel(\"Speedup\")\n",
    "    plt.xlabel(\"\")\n",
    "\n",
    "    plt.axhline(y=0, color=\"k\", lw=1)  # speedup line\n",
    "    plt.axvline(x=plt.xlim()[1] - 1, color=\"k\", lw=1, linestyle=\"--\")  # average line\n",
    "\n",
    "    ax.get_legend().set_title(\"\")  # no legend title\n",
    "    plt.legend(loc='upper right')\n",
    "    ax.get_legend().draw_frame(True)\n",
    "\n",
    "    # plot shape and size\n",
    "    figsize = (7, 3.2)\n",
    "    typecast = float\n",
    "\n",
    "    # counter negative offset:\n",
    "    ax.set_yticklabels([typecast(i) + 1 for i in ax.get_yticks()])\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "    viz.finalise(figsize=figsize, tight=True)\n",
    "else:\n",
    "    print(\"no new data, skipped\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Extending the Predictive Model\n",
    "\n",
    "Feature designers are bound to select as features only properties which are\n",
    "significant for the sparse benchmarks they test on, which can limit a model’s\n",
    "ability to generalize over a wider range of programs. We found this to be the\n",
    "case with the *Grewe et al.* model. The addition of automatically generated\n",
    "programs exposed two distinct cases where the model failed to generalize as a\n",
    "result of overspecializing to the NPB suite.\n",
    "\n",
    "The first case is that `F3` is sparse on many programs. This a result of the NPB\n",
    "implementation’s heavy exploitation of local memory buffers and the method by\n",
    "which they combined features (we speculate this was a necessary dimensionality\n",
    "reduction in the presence of sparse training programs). To counter this we\n",
    "extended the model to use the raw features in addition to the combined features.\n",
    "\n",
    "Distribution of feature values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F = [  # features\n",
    "    amd_clgen[\"F1:transfer/(comp+mem)\"],\n",
    "    amd_clgen[\"F2:coalesced/mem\"],\n",
    "    amd_clgen[\"F3:(localmem/mem)*avgws\"],\n",
    "    amd_clgen[\"F4:comp/mem\"]\n",
    "]\n",
    "nres = len(F[0])\n",
    "\n",
    "plt.subplots(1, 4)\n",
    "for i in range(len(F)):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.hist(F[i], bins=20)\n",
    "    plt.xlim(0,)\n",
    "    plt.xlabel(\"feature \" + str(i + 1))\n",
    "    nzeros = sum([1 for x in F[i] if x == 0])\n",
    "    print(\"ratio zero values feature {}: {:.3f} %\".format(i + 1, (nzeros / nres) * 100))\n",
    "\n",
    "viz.finalise(figsize=(9,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second case is that some of our generated programs had identical feature\n",
    "values as in the benchmark set, but had different *behavior* (i.e. optimal\n",
    "mappings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def same_features_different_behaviour(a, b, getfeatures):\n",
    "    \"\"\"returns true if features match but oracle device differs\"\"\"\n",
    "    af, bf = getfeatures(a), getfeatures(b)\n",
    "    ab, bb = a[\"oracle\"], b[\"oracle\"]\n",
    "    return ab != bb and np.array_equal(af, bf)\n",
    "\n",
    "print(\"Number of instances where CLgen kernels have identical features to benchmarks but different behaviour:\")\n",
    "matches, i, imax = defaultdict(int), 0, len(amd_benchmarks)\n",
    "for a in amd_benchmarks.to_dict('records'):\n",
    "    i += 1\n",
    "    print(\"\\r\", i, \"of\", imax, a[\"benchmark\"], end=\"\")\n",
    "    for b in amd_clgen.to_dict('records'):\n",
    "        if same_features_different_behaviour(a, b, cgo13.static_features):\n",
    "            matches[a[\"benchmark\"]] += 1\n",
    "\n",
    "print(\"\\r\" + clgen.format_json(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found this to be caused by the lack of discriminatory features for branching,\n",
    "since the NPB programs are implemented in a manner which aggressively minimized\n",
    "branching. To counter this we extended the predictive model with an additional\n",
    "feature with a static count of branching operations in a kernel.\n",
    "\n",
    "Speedups of our extended model across all seven of the benchmark suites used in Section 2:\n",
    "\n",
    "**Figure 8:** Speedups of predictions using our extended model over *Grewe et al.* on both experimental platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare_clfs(clf1, get_features1, clf2, get_features2, D1, D2, benchmark):\n",
    "    \"\"\"cross-validate across all benchmarks using CGO13 model and our own, with\n",
    "    and without synthetic benchmarks. Report per-platform speedup of our model\n",
    "    over CGO13\"\"\"\n",
    "    test1_mask = D1[\"benchmark\"].str.contains(r\"^\" + benchmark)\n",
    "    test2_mask = D2[\"benchmark\"].str.contains(r\"^\" + benchmark)\n",
    "    assert(len(D1[test1_mask]) == len(D2[test2_mask]))\n",
    "\n",
    "    # create data masks. For training we exclude all results from benchmark\n",
    "    train1_mask = ~test1_mask\n",
    "    train2_mask = ~test2_mask\n",
    "\n",
    "    # create training and testing data\n",
    "    X1_train = get_features1(D1.loc[train1_mask])\n",
    "    X2_train = get_features2(D2.loc[train2_mask])\n",
    "    y1_train = cgo13.getlabels(D1[train1_mask])\n",
    "    y2_train = cgo13.getlabels(D2[train2_mask])\n",
    "\n",
    "    D1_test = D1[test1_mask]\n",
    "    D2_test = D2[test2_mask]\n",
    "    X1_test = get_features1(D1.loc[test1_mask])\n",
    "    X2_test = get_features2(D2.loc[test2_mask])\n",
    "    y1_test = cgo13.getlabels(D1_test)\n",
    "    y2_test = cgo13.getlabels(D2_test)\n",
    "\n",
    "    clf1.fit(X1_train, y1_train)  # train classifiers\n",
    "    clf2.fit(X2_train, y2_train)\n",
    "\n",
    "    predicted1 = clf1.predict(X1_test)  # make predictions\n",
    "    predicted2 = clf2.predict(X2_test)\n",
    "\n",
    "    D_out = []\n",
    "    for d, y, p1, p2 in zip(D1_test.to_dict('records'), y1_test,\n",
    "                            predicted1, predicted2):\n",
    "        d[\"p1\"], d[\"p2\"] = p1, p2\n",
    "        D_out.append(d)\n",
    "\n",
    "    return D_out  # return a list of dicts\n",
    "\n",
    "\n",
    "aB = pd.read_csv(\"../data/amd-benchmarks.csv\")\n",
    "aB[\"synthetic\"] = np.zeros(len(aB))\n",
    "bB = pd.read_csv(\"../data/nvidia-benchmarks.csv\")\n",
    "bB[\"synthetic\"] = np.zeros(len(bB))\n",
    "B = pd.concat((aB, bB))\n",
    "\n",
    "aS = pd.read_csv(\"../data/amd-clgen.csv\")\n",
    "aS[\"synthetic\"] = np.ones(len(aS))\n",
    "bS = pd.read_csv(\"../data/nvidia-clgen.csv\")\n",
    "bS[\"synthetic\"] = np.ones(len(bS))\n",
    "S = pd.concat((aS, bS))\n",
    "\n",
    "aBS = pd.concat((aB, aS))\n",
    "bBS = pd.concat((bB, bS))\n",
    "BS = pd.concat((B, S))\n",
    "\n",
    "assert(len(B) == len(aB) + len(bB))  # sanity checks\n",
    "assert(len(S) == len(aS) + len(bS))\n",
    "assert(len(BS) == len(aBS) + len(bBS))\n",
    "\n",
    "# get benchmark names: <suite>-<benchmark>\n",
    "benchmark_names = sorted(set([\n",
    "    re.match(r\"^([^0-9]+-[0-9\\.]+-[^-]+)\", b).group(1)\n",
    "    for b in B[\"benchmark\"]\n",
    "]))\n",
    "\n",
    "# perform cross-validation\n",
    "B_out = []\n",
    "for i, benchmark in enumerate(benchmark_names):\n",
    "    print(\"\\ranalyzing\", i + 1, benchmark, end=\"\")\n",
    "    cgo13_clf, our_clf = cgo13.model(), get_our_model()\n",
    "    cgo13_features, our_features = get_cgo13_features, get_our_features\n",
    "\n",
    "    # cross validate on Grewe et al. and our model\n",
    "    tmp = compare_clfs(cgo13_clf, cgo13_features, our_clf, our_features,\n",
    "                       aBS, aBS, benchmark)\n",
    "    for d in tmp: d[\"platform\"] = \"AMD Tahiti 7970\"\n",
    "    B_out += tmp\n",
    "\n",
    "    # reset models\n",
    "    cgo13_clf, our_clf = cgo13.model(), get_our_model()\n",
    "\n",
    "    # same as before, on other platform:\n",
    "    tmp = compare_clfs(cgo13_clf, cgo13_features, our_clf, our_features,\n",
    "                       bBS, bBS, benchmark)\n",
    "    for d in tmp: d[\"platform\"] = \"NVIDIA GTX 970\"\n",
    "    B_out += tmp\n",
    "print()\n",
    "\n",
    "# create results frame\n",
    "R_out = []\n",
    "# get runtimes of device using predicted device\n",
    "for b in B_out:\n",
    "    p1_runtime = b[\"runtime_\" + b[\"p1\"].lower()]\n",
    "    p2_runtime = b[\"runtime_\" + b[\"p2\"].lower()]\n",
    "\n",
    "    # speedup is the ratio of runtime using our predicted device\n",
    "    # over runtime using CGO13 predicted device.\n",
    "    b[\"p_speedup\"] = p2_runtime / p1_runtime\n",
    "\n",
    "    # get the benchmark name\n",
    "    b[\"group\"] = escape_benchmark_name(b[\"benchmark\"])\n",
    "\n",
    "    R_out.append(b)\n",
    "R = pd.DataFrame(R_out)\n",
    "\n",
    "improved = R[R[\"p_speedup\"] > 1]\n",
    "\n",
    "Amask = R[\"platform\"] == \"AMD Tahiti 7970\"\n",
    "Bmask = R[\"platform\"] == \"NVIDIA GTX 970\"\n",
    "a = R[Amask]\n",
    "b = R[Bmask]\n",
    "\n",
    "a_speedups = a.groupby([\"group\"])[\"p_speedup\"].mean()\n",
    "b_speedups = b.groupby([\"group\"])[\"p_speedup\"].mean()\n",
    "\n",
    "a_speedup = labmath.mean(a_speedups)\n",
    "b_speedup = labmath.mean(b_speedups)\n",
    "\n",
    "assert(len(R) == len(a) + len(b))  # sanity-check\n",
    "\n",
    "print(\"  #. benchmarks:          \",\n",
    "      len(set(B[\"benchmark\"])), \"kernels,\", len(B), \"observations\")\n",
    "print(\"  #. synthetic:           \",\n",
    "      len(set(S[\"benchmark\"])), \"kernels,\", len(S), \"observations\")\n",
    "print()\n",
    "print(\"  Speedup on AMD:          {:.2f} x\".format(a_speedup))\n",
    "print(\"  Speedup on NVIDIA:       {:.2f} x\".format(b_speedup))\n",
    "\n",
    "palette = sns.cubehelix_palette(\n",
    "    len(set(R[\"platform\"])), start=4, rot=.8, light=.8, dark=.3)\n",
    "\n",
    "R = R.append({  # average bars\n",
    "    \"group\": \"Average\",\n",
    "    \"p_speedup\": a_speedup,\n",
    "    \"platform\": \"AMD Tahiti 7970\"\n",
    "}, ignore_index=True)\n",
    "R = R.append({\n",
    "    \"group\": \"Average\",\n",
    "    \"p_speedup\": b_speedup,\n",
    "    \"platform\": \"NVIDIA GTX 970\"\n",
    "}, ignore_index=True)\n",
    "\n",
    "R[\"p_speedup\"] -= 1  # negative offset so that bars start at 1\n",
    "\n",
    "ax = sns.barplot(x=\"group\", y=\"p_speedup\", hue=\"platform\", data=R,\n",
    "                 palette=palette, ci=None)\n",
    "\n",
    "plt.ylabel(\"Speedup over Grewe et al.\"); plt.xlabel(\"\")\n",
    "\n",
    "plt.axhline(y=0, color=\"k\", lw=1)\n",
    "plt.axvline(x=plt.xlim()[1] - 1, color=\"k\", lw=1, linestyle=\"--\")\n",
    "plt.ylim(-1, 9)\n",
    "plt.setp(ax.get_xticklabels(), rotation=90)  # rotate x ticks\n",
    "ax.get_legend().set_title(\"\")  # legend\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# counter negative offset\n",
    "ax.set_yticklabels([int(i) + 1 for i in ax.get_yticks()])\n",
    "\n",
    "ax.get_legend().draw_frame(True)\n",
    "\n",
    "viz.finalise(figsize=(9, 4), tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performance, even on this tenfold increase of benchmarks, is good. There\n",
    "are three benchmarks on which the model performs poorly: `gemmv`, `hotspot`, and\n",
    "`pathfinder`. Each of those programs make heavy use of loops, which we believe\n",
    "the static code features of the model fail to capture. This could be addressed\n",
    "by extracting dynamic instruction counts using profiling, but we considered this\n",
    "beyond the scope of our work. It is not our goal to perfect the predictive\n",
    "model, but to show the performance improvements associated with training on\n",
    "synthetic programs. To this extent, we are successful, achieving average\n",
    "speedups of $3.56\\times$ on AMD and $5.04\\times$ on NVIDIA across a very large\n",
    "test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this experiment using the small subset of experimental data we produced earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare_clfs(clf1, get_features1, clf2, get_features2, D1, D2, benchmark):\n",
    "    \"\"\"cross-validate across all benchmarks using CGO13 model and our own, with\n",
    "    and without synthetic benchmarks. Report per-platform speedup of our model\n",
    "    over CGO13\"\"\"\n",
    "    test1_mask = D1[\"benchmark\"].str.contains(r\"^\" + benchmark)\n",
    "    test2_mask = D2[\"benchmark\"].str.contains(r\"^\" + benchmark)\n",
    "    assert(len(D1[test1_mask]) == len(D2[test2_mask]))\n",
    "\n",
    "    # create data masks. For training we exclude all results from benchmark\n",
    "    train1_mask = ~test1_mask\n",
    "    train2_mask = ~test2_mask\n",
    "\n",
    "    # create training and testing data\n",
    "    X1_train = get_features1(D1.loc[train1_mask])\n",
    "    X2_train = get_features2(D2.loc[train2_mask])\n",
    "    y1_train = cgo13.getlabels(D1[train1_mask])\n",
    "    y2_train = cgo13.getlabels(D2[train2_mask])\n",
    "\n",
    "    D1_test = D1[test1_mask]\n",
    "    D2_test = D2[test2_mask]\n",
    "    X1_test = get_features1(D1.loc[test1_mask])\n",
    "    X2_test = get_features2(D2.loc[test2_mask])\n",
    "    y1_test = cgo13.getlabels(D1_test)\n",
    "    y2_test = cgo13.getlabels(D2_test)\n",
    "\n",
    "    clf1.fit(X1_train, y1_train)  # train classifiers\n",
    "    clf2.fit(X2_train, y2_train)\n",
    "\n",
    "    predicted1 = clf1.predict(X1_test)  # make predictions\n",
    "    predicted2 = clf2.predict(X2_test)\n",
    "\n",
    "    D_out = []\n",
    "    for d, y, p1, p2 in zip(D1_test.to_dict('records'), y1_test,\n",
    "                            predicted1, predicted2):\n",
    "        d[\"p1\"], d[\"p2\"] = p1, p2\n",
    "        D_out.append(d)\n",
    "\n",
    "    return D_out  # return a list of dicts\n",
    "\n",
    "if can_reproduce_experiments():\n",
    "    B = pd.read_csv(\"../data/benchmarks/training.csv\")\n",
    "    B[\"synthetic\"] = np.zeros(len(B))\n",
    "\n",
    "    S = pd.read_csv(\"../data/clgen-10/training.csv\")\n",
    "    S[\"synthetic\"] = np.ones(len(S))\n",
    "\n",
    "    BS = pd.concat((B, S))\n",
    "\n",
    "    assert(len(BS) == len(B) + len(S))\n",
    "\n",
    "    # get benchmark names: <suite>-<benchmark>\n",
    "    benchmark_names = sorted(set([\n",
    "        re.match(r\"^([^0-9]+-[0-9\\.]+-[^-]+)\", b).group(1)\n",
    "        for b in B[\"benchmark\"]\n",
    "    ]))\n",
    "\n",
    "    # perform cross-validation\n",
    "    B_out = []\n",
    "    for i, benchmark in enumerate(benchmark_names):\n",
    "        print(\"\\ranalyzing\", i + 1, benchmark, end=\"\")\n",
    "        cgo13_clf, our_clf = cgo13.model(), get_our_model()\n",
    "        cgo13_features, our_features = get_cgo13_features, get_our_features\n",
    "\n",
    "        # cross validate on Grewe et al. and our model\n",
    "        tmp = compare_clfs(cgo13_clf, cgo13_features, our_clf, our_features,\n",
    "                           BS, BS, benchmark)\n",
    "        B_out += tmp\n",
    "    print()\n",
    "\n",
    "    # create results frame\n",
    "    R_out = []\n",
    "    # get runtimes of device using predicted device\n",
    "    for b in B_out:\n",
    "        p1_runtime = b[\"runtime_\" + b[\"p1\"].lower()]\n",
    "        p2_runtime = b[\"runtime_\" + b[\"p2\"].lower()]\n",
    "\n",
    "        # speedup is the ratio of runtime using our predicted device\n",
    "        # over runtime using CGO13 predicted device.\n",
    "        b[\"p_speedup\"] = p2_runtime / p1_runtime\n",
    "\n",
    "        # get the benchmark name\n",
    "        b[\"group\"] = escape_benchmark_name(b[\"benchmark\"])\n",
    "\n",
    "        R_out.append(b)\n",
    "    R = pd.DataFrame(R_out)\n",
    "\n",
    "    improved = R[R[\"p_speedup\"] > 1]\n",
    "\n",
    "    speedups = R.groupby([\"group\"])[\"p_speedup\"].mean()\n",
    "    speedup = labmath.mean(speedups)\n",
    "\n",
    "    print(\"  #. benchmarks:          \",\n",
    "          len(set(B[\"benchmark\"])), \"kernels,\", len(B), \"observations\")\n",
    "    print(\"  #. synthetic:           \",\n",
    "          len(set(S[\"benchmark\"])), \"kernels,\", len(S), \"observations\")\n",
    "    print()\n",
    "    print(\"  Speedup:                 {:.2f} x\".format(speedup))\n",
    "\n",
    "    palette = sns.cubehelix_palette(1, start=4, rot=.8, light=.8, dark=.3)\n",
    "\n",
    "    R = R.append({  # average bar\n",
    "        \"group\": \"Average\",\n",
    "        \"p_speedup\": speedup\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    R[\"p_speedup\"] -= 1  # negative offset so that bars start at 1\n",
    "\n",
    "    ax = sns.barplot(x=\"group\", y=\"p_speedup\", data=R,\n",
    "                     palette=palette, ci=None)\n",
    "\n",
    "    plt.ylabel(\"Speedup over Grewe et al.\"); plt.xlabel(\"\")\n",
    "\n",
    "    plt.axhline(y=0, color=\"k\", lw=1)\n",
    "    plt.axvline(x=plt.xlim()[1] - 1, color=\"k\", lw=1, linestyle=\"--\")\n",
    "    plt.ylim(-1, 9)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90)  # rotate x ticks\n",
    "\n",
    "    # counter negative offset\n",
    "    ax.set_yticklabels([int(i) + 1 for i in ax.get_yticks()])\n",
    "\n",
    "    viz.finalise(figsize=(7, 3.7), tight=True)\n",
    "else:\n",
    "    print(\"no new data, skipped\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Comparison of Source Features\n",
    "\n",
    "As demonstrated in Section 2, the predictive quality of a model for a given\n",
    "point in the feature space is improved with the addition of observations from\n",
    "neighboring points. By producing thousands of artificial programs modeled on the\n",
    "structure real OpenCL programs, CLgen is able to consistently and automatically\n",
    "generate programs which are close in the feature space to the benchmarks which\n",
    "we are testing on.\n",
    "\n",
    "To quantify this effect we use the static code features, plus the branching\n",
    "feature discussed in the previous subsection, to measure the number of CLgen\n",
    "kernels generated with the same feature values as those of the benchmarks we\n",
    "examined in the previous subsections. We examine only static code features to\n",
    "allow comparison with the GitHub kernels for which we have no automated method\n",
    "to execute them and extract runtime features, and CLSmith generated programs.\n",
    "\n",
    "Plotting the number of matches as a function of the number of kernels:\n",
    "\n",
    "**Figure 9:** The number of kernels from GitHub, CLSmith, and CLgen with static code features matching the benchmarks. CLgen generates kernels that are closer in the feature space than CLSmith, and can continue to do so long after we have exhausted the extent of the GitHub dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def uniq_benchmarks(g):\n",
    "    \"\"\"return set of unique benchmarks, removes duplicates\"\"\"\n",
    "    uniq, out = set(), []\n",
    "    for row in g.to_dict('records'):\n",
    "        key = (row['benchmark'])\n",
    "        if key not in uniq:\n",
    "            out.append(row)\n",
    "            uniq.add(key)\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "np.random.seed(204)\n",
    "\n",
    "# load datasets\n",
    "CLgen = pd.read_csv(\"../data/clgen-features.csv\")\n",
    "GitHub = pd.read_csv(\"../data/github-features.csv\")\n",
    "CLSmith = pd.read_csv(\"../data/clsmith-features.csv\")\n",
    "\n",
    "# extract features\n",
    "Benchmarks_features = cgo13.get_static_features(uniq_benchmarks(amd_benchmarks))\n",
    "CLgen_features = cgo13.get_static_features(CLgen)\n",
    "GitHub_features = cgo13.get_static_features(GitHub)\n",
    "CLSmith_features = cgo13.get_static_features(CLSmith)\n",
    "\n",
    "stepMin, stepMax = 0, 10000\n",
    "linStepSize = int((stepMax - stepMin) / 20)\n",
    "closeness_threshold = 0\n",
    "\n",
    "steps = np.array(list(range(stepMin, stepMax + linStepSize, linStepSize)),\n",
    "                 dtype=np.int32)\n",
    "steps[0] = max(steps[0], 1)\n",
    "\n",
    "GitHubSteps = [n for n in steps if n <= len(GitHub_features)]\n",
    "CLSmithSteps = [n for n in steps if n <= len(CLSmith_features)]\n",
    "CLgenSteps = [n for n in steps if n <= len(CLgen_features)]\n",
    "\n",
    "numGitHubSteps = len(GitHubSteps)\n",
    "numCLSmithSteps = len(CLSmithSteps)\n",
    "numCLgenSteps = len(CLgenSteps)\n",
    "\n",
    "print(\"#. GitHub kernels \", GitHubSteps[-1])\n",
    "print(\"#. CLSmith kernels\", CLSmithSteps[-1])\n",
    "print(\"#. CLgen kernels  \", CLgenSteps[-1])\n",
    "\n",
    "datasets = [\n",
    "    {\n",
    "        \"name\": \"GitHub\",\n",
    "        \"color\": \"b\",\n",
    "        \"linestyle\": \":\",\n",
    "        \"steps\": GitHubSteps,\n",
    "        \"static_features\": GitHub_features,\n",
    "        \"distances\": np.zeros(numGitHubSteps, dtype=float),\n",
    "        \"errors\": np.zeros(numGitHubSteps, dtype=float)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CLSmith\",\n",
    "        \"color\": \"r\",\n",
    "        \"linestyle\": \"--\",\n",
    "        \"steps\": CLSmithSteps,\n",
    "        \"static_features\": CLSmith_features,\n",
    "        \"distances\": np.zeros(numCLSmithSteps, dtype=float),\n",
    "        \"errors\": np.zeros(numCLSmithSteps, dtype=float)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CLgen\",\n",
    "        \"color\": \"g\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"steps\": CLgenSteps,\n",
    "        \"static_features\": CLgen_features,\n",
    "        \"distances\": np.zeros(numCLgenSteps, dtype=float),\n",
    "        \"errors\": np.zeros(numCLgenSteps, dtype=float)\n",
    "    },\n",
    "]\n",
    "\n",
    "numIterations = 10  # repeat experiment N times\n",
    "for dataset in datasets:\n",
    "    for i, n in enumerate(dataset[\"steps\"]):\n",
    "        numSteps = len(dataset[\"steps\"])\n",
    "        print(\"\\r\", dataset[\"name\"], i + 1, \"of\", numSteps, end=\"\")\n",
    "        r = np.zeros(numIterations, dtype=float)\n",
    "        for j in range(numIterations):\n",
    "            np.random.shuffle(dataset[\"static_features\"])\n",
    "            distances = get_nearest_neighbour_distance(\n",
    "                dataset[\"static_features\"][:n], Benchmarks_features)\n",
    "            r[j] = sum([1 if d <= closeness_threshold else 0 for d in distances])\n",
    "        dataset[\"distances\"][i] = np.mean(r)\n",
    "        dataset[\"errors\"][i] = np.std(r)\n",
    "    plt.errorbar(dataset[\"steps\"], dataset[\"distances\"],\n",
    "                 yerr=dataset[\"errors\"], c=dataset[\"color\"],\n",
    "                 linestyle=dataset[\"linestyle\"],\n",
    "                 label=dataset[\"name\"], ecolor=\"k\", capthick=1)\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "ax.get_legend().set_title(\"\")\n",
    "ax.get_legend().draw_frame(True)\n",
    "\n",
    "plt.xlim(stepMin, stepMax)\n",
    "plt.ylim(0,)\n",
    "plt.xlabel(\"#. kernels\"); plt.ylabel(\"#. matches\")\n",
    "\n",
    "viz.finalise(figsize=(4.5, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 10,000 unique CLgen kernels, more than a third have static feature values\n",
    "matching those of the benchmarks, providing on average 14 CLgen kernels for each\n",
    "benchmark. This confirms our original intuition: CLgen kernels, by emulating the\n",
    "way real humans write OpenCL programs, are concentrated in the same area of the\n",
    "feature space as real programs. Moreover, the number of CLgen kernels we\n",
    "generate is unbounded, allowing us to continually refine the exploration of the\n",
    "feature space, while the number of kernels available on GitHub is finite.\n",
    "CLSmith rarely produces code similar to real-world OpenCL programs, with only\n",
    "0.53% of the generated kernels have matching feature values with benchmark\n",
    "kernels. We conclude that the unique contribution of CLgen is its ability to\n",
    "generate many thousands of programs *that are appropriate for predictive\n",
    "modeling*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Related Work\n",
    "\n",
    "See the PDF version of this paper.\n",
    "\n",
    "# 10. Conclusion\n",
    "\n",
    "The quality of predictive models is bound by the quantity and quality of\n",
    "programs used for training, yet there is typically only a few dozen common\n",
    "benchmarks available for experiments. We present a novel tool which is the first\n",
    "of it’s kind — an entirely probabilistic program generator capable of generating\n",
    "an unbounded number of human like programs. Our approach applies deep learning\n",
    "over a huge corpus of publicly available code from GitHub to automatically infer\n",
    "the semantics and practical usage of a programming language. Our tool generates\n",
    "programs which to trained eyes are indistinguishable from hand-written code. We\n",
    "tested our approach using a state of the art predictive model, improving its\n",
    "performance by a factor of 1.27x. We found that synthetic benchmarks exposed\n",
    "weaknesses in the feature set which, when corrected, further improved the\n",
    "performance by 2.66x. In future work we will extend CLgen to synthesize\n",
    "benchmarks in multiple programming languages, and investigate methods for\n",
    "performing an automatic directed search of the program space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Evaluation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLgen",
   "language": "python",
   "name": "clgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
