{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifact Evaluation: Synthesizing Benchmarks for Predictive Modeling\n",
    "\n",
    "[Chris Cummins](http://chriscummins.cc/),\n",
    "[Pavlos Petoumenos](http://homepages.inf.ed.ac.uk/ppetoume/),\n",
    "[Zheng Wang](http://www.lancaster.ac.uk/staff/wangz3/),\n",
    "[Hugh Leather](http://homepages.inf.ed.ac.uk/hleather/).\n",
    "\n",
    "<span style=\"color:#f00;\">**IMPORTANT!**</span> Changes to this document are persistent. Before doing anything else, select from the menu \"File\" > \"Make a Copy\". This will prevent your changes from affecting other users. Thank you.\n",
    "\n",
    "High system load may lead to inconsistent performance results; this may occur if multiple reviewers are accessing the server simultaneously.\n",
    "\n",
    "### How to use this document\n",
    "\n",
    "1. Click on the first code block.\n",
    "1. Press `Ctrl+Enter` to run the code.\n",
    "1. Once completed, the code will self-test. If the test passes it will display:\n",
    "<div style=\"background-color:#5cb85c; color:#fff; text-align:center; border-radius:10px;\">\n",
    "  <h1 style=\"padding:.5em; font-weight:400;\">☑ Complete</h1>\n",
    "</div>\n",
    "If the test fails it will display:\n",
    "<div style=\"background-color:#d9534f; color:#fff; text-align:center; border-radius:10px;\">\n",
    "  <h1 style=\"padding:.5em; font-weight:400;\">☒ Failed</h1>\n",
    "</div>\n",
    "1. Evaluate the output and proceed to the next code block.\n",
    "\n",
    "Alternatively, run all of the code blocks automatically in sequence by selecting from the menu \"Kernel\" > \"Restart and Run All\".\n",
    "\n",
    "For further information on using Jupyter notebooks, see the [official documentation](https://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Notebook%20Basics.html).\n",
    "\n",
    "### Resources\n",
    "* [\"Interactive Paper\"](/notebooks/Paper.ipynb) is a comprehensive version of this evaluation for those wishing to evaluate every aspect of the paper.\n",
    "* Install this artifact on your own hardware: http://chriscummins.cc/cgo17/\n",
    "* Online version of the OpenCL Turing Test: http://humanorrobot.uk/game/?g=opencl&m=nitt\n",
    "* CLgen source code: https://github.com/ChrisCummins/clgen/\n",
    "* CLgen API documentation: http://chriscummins.cc/clgen/api/\n",
    "\n",
    "Here is the first code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preamble\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%run lib/preamble.py\n",
    "\n",
    "complete(msg=\"Initial setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Setup\n",
    "\n",
    "This artifact must be evaluated on a CPU-GPU heterogeneous system. In the paper, we used:\n",
    "* **Intel Core i7-3820**\n",
    "* **AMD Tahiti 7970**\n",
    "* **NVIDIA GTX 970**\n",
    "\n",
    "Details about this system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import clgen.clutil\n",
    "clgen.clutil.platform_info()\n",
    "\n",
    "import random\n",
    "uid = random.randint(0, 100000)\n",
    "fs.rm(\"../data/usr/{uid}\".format(uid=uid))\n",
    "fs.mkdir(\"../data/usr/{uid}/clgen\".format(uid=uid))\n",
    "fs.mkdir(\"../data/usr/{uid}/benchmarks\".format(uid=uid))\n",
    "print(\"\\nUnique test ID:\", uid)\n",
    "\n",
    "complete(can_reproduce_experiments(), \"Artifact is running on suitable hardware, please check system load\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesizing Programs with CLgen\n",
    "Load our pre-trained Neural Network, generate new programs, validate samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The model used in the paper (pre-trained):\")\n",
    "model = clgen.model.from_tar(\"../data/clgen-github-model-2016-nov-2048x3.tar.bz2\")\n",
    "print(model)\n",
    "complete(model.hash == \"f2fb3ad753896d54fe284c138eaa703db3518bbb\",\n",
    "         \"Load pre-trained neural network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample model\n",
    "import clgen.sampler\n",
    "import clgen.dbutil\n",
    "import clgen.explore\n",
    "argspec = ['__global float*', '__global float*', '__global float*', 'const int']\n",
    "sampler = clgen.sampler.from_json({\n",
    "        \"kernels\": { \n",
    "            \"args\": argspec,\n",
    "            \"max_length\": 5000,\n",
    "        },\n",
    "        \"sampler\": {\n",
    "            \"batch_size\": 25,\n",
    "            \"max_kernels\": 10,\n",
    "        }\n",
    "    })\n",
    "\n",
    "print(\"Sample from the model used in the paper:\\n\")\n",
    "print(\"Seed text:\", clgen.sampler.serialize_argspec(argspec))\n",
    "sampler.cache(model).empty()\n",
    "sampler.sample(model)\n",
    "\n",
    "db = sampler.cache(model)[\"kernels.db\"]\n",
    "num_good_kernels = clgen.dbutil.num_good_kernels(db)\n",
    "clgen.explore.explore(db)\n",
    "complete(num_good_kernels >= 5,\n",
    "         \"Generated {} OpenCL kernels\".format(num_good_kernels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Generated kernels\\n\")\n",
    "try:\n",
    "    db = clgen.dbutil.connect(sampler.cache(model)[\"kernels.db\"])\n",
    "    c = db.cursor()\n",
    "\n",
    "    c.execute(\"\"\"SELECT Contents FROM PreprocessedFiles WHERE status=0\"\"\")\n",
    "    for i, row in enumerate(c.fetchall()):\n",
    "        kernel = row[0]\n",
    "        print(\"\\nKernel \", i+1, \":\\n\", sep=\"\")\n",
    "        print(kernel)\n",
    "\n",
    "    c.close(); db.close()\n",
    "    complete(msg=\"Display generated OpenCL kernels\")\n",
    "except:\n",
    "    complete(False, \"Failed to display generated OpenCL kernels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark suite performance results\n",
    "Generate new runtimes using 1 of the 7 benchmark suites used in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"running ...  (this will take a few minutes)\")\n",
    "try:\n",
    "    !rm -f ../data/benchmarks/*.csv ../data/benchmarks/timestamp.csv\n",
    "    !cd benchmarks && ./mkdata\n",
    "    data = pd.read_csv(\"../data/benchmarks/training.csv\")\n",
    "    benchmarks_timestamp = readfile(\"../data/benchmarks/timestamp.txt\")\n",
    "    move(\"../data/benchmarks/training.csv\", \"../data/usr/{uid}/benchmarks/\".format(uid=uid))\n",
    "    move(\"../data/benchmarks/timestamp.txt\", \"../data/usr/{uid}/benchmarks/\".format(uid=uid))\n",
    "    complete(len(data) == 17, \"Produced new performance results for benchmarks\")\n",
    "except:\n",
    "    complete(False, \"Did not produce new performance results for benchmarks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    if benchmarks_timestamp != readfile(\"../data/usr/{uid}/benchmarks/timestamp.txt\".format(uid=uid)):\n",
    "        print(\"warning: data timestamp has changed, please re-run experiments\", file=sys.stderr)\n",
    "    data = pd.read_csv(\"../data/usr/{uid}/benchmarks/training.csv\".format(uid=uid))\n",
    "    ax = sns.barplot(x=\"benchmark\", y=\"speedup\", data=data)\n",
    "    plt.title(\"Runtimes generated \" + benchmarks_timestamp)\n",
    "    plt.ylabel(\"Max speedup\")\n",
    "    plt.xlabel(\"AMD SDK Benchmark kernels\")\n",
    "    plt.axhline(y=1, color=\"k\", lw=1)  # speedup line\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90)  # rotate x ticks\n",
    "    ax.set_xticklabels([shortbenchmark(x.get_text()) for x in ax.get_xticklabels()])\n",
    "    viz.finalise(figsize=(9,4))\n",
    "    complete(len(set(data[\"benchmark\"])) == 17, \"New performance numbers from 17 AMD kernels\")\n",
    "except:\n",
    "    complete(False, \"Failed to analyze benchmark results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLgen kernel performance results\n",
    "Generate new runtimes using 1% of CLgen kernels used in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(\"running ...  (this will take a few minutes)\")\n",
    "# try:\n",
    "!rm -f ../data/clgen-10/*.csv ../data/clgen-10/timestamp.txt\n",
    "!cd bin && ./mkdata\n",
    "data = pd.read_csv(\"../data/clgen-10/training.csv\")\n",
    "clgen_timestamp = readfile(\"../data/clgen-10/timestamp.txt\")\n",
    "move(\"../data/clgen-10/training.csv\", \"../data/usr/{uid}/clgen/\".format(uid=uid))\n",
    "move(\"../data/clgen-10/timestamp.txt\", \"../data/usr/{uid}/clgen/\".format(uid=uid))\n",
    "complete(len(set(data[\"benchmark\"])) == 17, \"Produced new performance results for CLgen benchmarks\")\n",
    "# except:\n",
    "#     complete(False, \"Did not produce new performance results for CLgen benchmarks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    if clgen_timestamp != readfile(\"../data/usr/{uid}/clgen/timestamp.txt\".format(uid=uid)):\n",
    "        print(\"warning: data timestamp has changed, please re-run experiments\", file=sys.stderr)\n",
    "\n",
    "    data = pd.read_csv(\"../data/usr/{uid}/clgen/training.csv\".format(uid=uid))   \n",
    "    ax = sns.barplot(x=\"benchmark\", y=\"speedup\", ci=95, data=data)\n",
    "    plt.title(\"Runtimes generated \" + clgen_timestamp)\n",
    "    plt.ylabel(\"Max speedups (95% CI across datasets)\")\n",
    "    plt.xlabel(\"CLgen kernels\")\n",
    "    plt.axhline(y=1, color=\"k\", lw=1)  # speedup line\n",
    "    ax.set_xticklabels(range(1, len(data) + 1))\n",
    "    viz.finalise(figsize=(9,4))\n",
    "    complete(len(set(data[\"benchmark\"])) == 17, \"New performance numbers from 17 CLgen kernels\")\n",
    "except:\n",
    "    complete(False, \"Failed to analyze CLgen benchmark results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Model performance using CLgen\n",
    "Test predictive model performance with and without additional CLgen kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    header(\"Results from the paper on AMD\")\n",
    "    plot_speedups_with_clgen(\"../data/amd-benchmarks.csv\", \"../data/amd-clgen.csv\", suite=\"npb\")\n",
    "\n",
    "    header(\"Results from the paper on NVIDIA\")\n",
    "    plot_speedups_with_clgen(\"../data/nvidia-benchmarks.csv\", \"../data/nvidia-clgen.csv\", suite=\"npb\")\n",
    "\n",
    "    header(\"Results using runtimes generated: Benchmarks\",\n",
    "           readfile(\"../data/usr/{uid}/benchmarks/timestamp.txt\".format(uid=uid)), \"- CLgen\",\n",
    "           readfile(\"../data/usr/{uid}/clgen/timestamp.txt\".format(uid=uid)))\n",
    "    a, b = plot_speedups_with_clgen(\"../data/usr/{uid}/benchmarks/training.csv\".format(uid=uid),\n",
    "                                    \"../data/usr/{uid}/clgen/training.csv\".format(uid=uid), suite=\"amd\")\n",
    "    complete(b > a, \"Predictive mode performance improves with CLgen kernels by {:.0f}%\".format((b / a) * 100 - 100))\n",
    "except:\n",
    "    complete(False, \"Failed to generate data for predictive model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Predictive Model\n",
    "Compare performance of extended predictive model over *Grewe et al*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    header(\"Results from the paper\")\n",
    "    plot_speedups_extended_model_2platform((\"../data/amd-benchmarks.csv\", \"../data/amd-clgen.csv\"),\n",
    "                                           (\"../data/nvidia-benchmarks.csv\", \"../data/nvidia-clgen.csv\"))\n",
    "\n",
    "    header(\"Results using new data\")\n",
    "    speedup = plot_speedups_extended_model(\"../data/usr/{uid}/benchmarks/training.csv\".format(uid=uid),\n",
    "                                           \"../data/usr/{uid}/clgen/training.csv\".format(uid=uid))\n",
    "    complete(speedup >= 1.0, \"Extended predictie model improves performance by {:.0f}%\".format(speedup * 100 - 100))\n",
    "except:\n",
    "    complete(False, \"Failed to generate data for extended predictive model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the minimal Artifact Evaluation experiments. For a much more comprehensive evaluation of our work, including analysis of OpenCL rewriting, training neural networks, and validating kernel beaviour, see:\n",
    "# [Interactive Paper](/notebooks/Paper.ipynb)\n",
    "\n",
    "\n",
    "### Resources\n",
    "* Install this artifact on your own hardware: http://chriscummins.cc/cgo17/\n",
    "* Online version of the OpenCL Turing Test: http://humanorrobot.uk/game/?g=opencl&m=nitt\n",
    "* CLgen source code: https://github.com/ChrisCummins/clgen/\n",
    "* CLgen API documentation: http://chriscummins.cc/clgen/api/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLgen",
   "language": "python",
   "name": "clgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
